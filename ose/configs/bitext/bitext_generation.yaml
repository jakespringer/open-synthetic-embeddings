model_config:
  model_type: "vllm_instruct"
  model_path: "meta-llama/Llama-3.1-8B-Instruct"
  default_system_prompt: "You are a helpful assistant."
  engine_kwargs:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_model_len: 4096
    dtype: "bfloat16"
  concurrency: 8
  batch_size: 8
  default_sampling_params:
    temperature: 0.8
    top_p: 0.9
    max_tokens: 4096

schema_config:
  schema_file: "ose/schema/bitext.json"
  extra_args: null
  extra_args_p: null

n_samples: 100
output_path: "output/bitext_samples.jsonl"
seed: 42
overwrite_output: false
